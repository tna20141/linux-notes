
FOREWORD
========

This journal is my personal research journal that records questions, answers, discoveries, ideas,
opninions and similar things in the journey to understand the Linux kernel. I didn't start writing
this journal from the beginning, but just sometime after I decided to go for the kernel. The
journal will be very simple (at least at first). The only format to it is that I will keep track
of time (in date) in this journal. The content will be as intuitive as possible, no formal
structure is needed. Now this is supposed to be a (live) journal, so I'm not gonna summerize things
I've learned so far here, because they are more suitable to be in a note.

========

Jul 21st 2013

I started the journal today because I think it's important for the road ahead.

Kinda done with the paging init, moving to actual memory management (chapter 8 of ULK).

I think 64-bit architecture doesn't have ZONE_HIGHMEM because the address space that 64-bit can
handle is just too big, thus it makes no sense to partition the RAM into many zones.

Page frames are allocated in the order of 2, this might be due to the buddy system's mechanism.

========

Jul 22nd 2013

Seems like mem_map array contains info about all page frames (fixed sized array), so the code
__va((unsigned long)(page - mem_map)<<12) is possible. I mean the page frame index corresponds to
the linear address.

Page mapping is all about mapping a PAGE FRAME into a LINEAR ADDRESS. What is in the page frame
depends on other things (like a loading routine or an allocation, I guess).

It's often said that in x86, memory beyond the 896MB mark is high memory. I wondered if the 128MB
beyond that belonged to high memory or still inside the kernel address space. A logical answer is
that the 128MB of the kernel is just the LINEAR address space, not the physical one. So the 128MB
linear address space is used by the kernel for the stuff it's supposed to be used for, and the
128MB physical address space beyond the 896MB mark is genuine high memory.

I'll come back to chapter 3 (Processes) for now. I think reading sequentially is still better.

========

Jul 25th 2013

About handling process wait queues. The book often quotes the following code snippet:
if (!condition)
	schedule();
Doesn't this contain a potential race condition? I'm pretty sure they took care of this (since this
is big stuff), but I don't know how.

========

Aug 19th 2013

I think a process switch doesn't need to save GPRs (general purpose registers) because (most of)
these are preserved over a function call anyway. Other things like PGD addresses (in cr3) are stored
in process descriptor, so no need to save them either.

========

Aug 27th 2013

If the CLONE_THREAD flag is set, tsk->exit_signal is initialized to -1, which means that the (child)
process won't send any signal to its parent when it terminates. This makes sense, since only the
last member of the thread group (usually, the thread group leader) gets to send the exit signal to
the parent. And since the CLONE_THREAD flag is specified, the in-dicussed process is inserted into
its parent's thread group, which means it won't be a thread group leader.

The interrupt handler can't use the interrupted process as its backing process (in other words,
its "schedule agent") for rescheduling, probably because the interrupted process is already busy
saving its states from the interrupt handler, it can't also contain the interrupt handler's states.
It cant be thought of as follow: the interrupted process is already annoyed that the interrupt
handler takes up its kernel stack, it won't be any kinder as to back the handler up for
rescheduling. Also making interrupt handlers sleepable is gonna get things very much complicated.

I still don't know exactly why interrupt handler can't sleep (maybe it shouldn't, but there's no
concrete reason why it CAN'T).

I think I know why. The interrupt handler has NOTHING TO DO with the process. Aside from using the
same stack (sometimes they don't even do that), they should be as decoupled as possible. Putting
the process to sleep because of the handler (when the process itself is perfectly runnable) is...
bad, I supposed. So, interrupt handler shouldn't be blocked.

I guess that makes Robert's explanation somewhat reasonable (with all that "no backing process" and
"how would it reschedule?" stuff). But Daniel & Marco's "can't block because they are nested"
doesn't seem to relate to this. Maybe they mean that an inner handler sleeping might block the outer
handler as well, and that's not good.

OK, we're done here. This is satisfying enough for me (and for now).

========

Sep 1st 2013

I'm at the beginning of chapter 5 (about kernel preemption). I figure the kernel can allocate time
to processes by means of hardware timer. When a timer expires, it issues an interrupt to the CPU,
whose purpose is to inform the kernel that the allocated time is up and a new process should be
scheduled to run next.

========

Sep 8th 2013

DAMN! All this time I thought unlikely(cond) equals (!cond), and from
http://stackoverflow.com/questions/109710/likely-unlikely-macros-in-the-linux-kernel it says that
unlikely(cond) essentially means (cond). likely or unlikely is just for telling if the condition
is likely to happen or not, it has nothing to do with the real logic. So I could've mistaken a lot
of stuff along the way because of this. Really, damn!!!

And today is her birthday...

========

Feb 15th 2014

It's been a while since I last used this journal.

I've decided that this journal is for writing logs about TODO stuff, events,... and won't include
much (if any) technical knowledge or information. Those will be documented in other text files.
Ok, now continues the normal logging.

Tomorrow, I need to examine do_exit() code carefully.

========

Feb 16th 2014

Will have to come back to do_exit() later, when I will have understood signals and the whole
process structure. In fact, I need to recrunch most of the chapter.

I created a github repos for these notes today. Just feel like doing it.

Never heard of the concept "pid namespace". This will be tricky. I need to look into this.
Also, there's another feature called "control group".

========

Feb 18th 2014

I get to know the infamous "F0 0F" bug. An example is the "F0 0F C7 C8" instruction. Basically
it's tricking the CPU to do 2 consecutive reads when it's supposed to do a read-update-write
instruction (with the lock prefix in action). The second read is because the instruction is
purposefully made invalid, so an exception handler is called and the CPU tries to read its address.
When this happens the CPU is confused and it just hangs. This bug is easily crafted, like so:
char a[4] = {0xf0, 0x0f, 0xc7, 0xc8}; void (*f)() = a; f();
This bug is serious as it requires no privileges. Attempts to fix this bug mostly involves
triggering page fault, since page fault disables the lock byte.

========

Feb 22nd 2014

The issue of  whether masking an interrupt line disable it for the entire system or just the local
CPU  really bugs me. [Understanding LK] seems to say masking only disables for the local CPU, but
[LKD] declares firmly that masking disables for all CPUs (because the line is disabled in the
interrupt controller itself). Maybe the 2 types of masking are not the same.

========

Feb 25th 2014

I need to draw the stack state each time I see the "saving to stack" thing.

Coming back to preempt_schedule_irq() in chapter 5.

========

Feb 26th 2014

Another problem arises. This time with workqueues. In [2.6.34.7] code, the work_func_t type is
defined as void (*work_func_t)(struct work_struct *work), meaning it takes the hole work_struct
as data argument. However, the books (and my thinking) say that it needs "void *data" as argument,
and the work_struct doesn't even have a data pointer. Actually it has a field named "data", but
this field, quote from the source "work queue pointer and the flags rolled into one". Where on
earth could the work queue function find its data, and the hell with work_struct's data.

========

Mar 04th 2014

spin_lock_irqsave() <-- I dont know if this function allows interrupt while spinning, or it
disables interrupt (and preemption) right from the start and keep it down all the way until the
lock is enabled. From examining the source, I would say the latter case is true.

I often find that calling schedule() on a condition is not atomic. What if a condition becomes
false right after it is checked and right before schedule() is invoked? Also, a wakeup call could
also be missed (if the call happens right before the schedule() invocation).

========

Mar 08th 2014

About the schedule() problem above. I assume there might be a solution for it as below. The wake
function and schedule(), each has a section that doesn't interleave with each other. For example,
the wake function has a section called A, schedule() has a section called B. If A occurs before B,
schedule() will see that current is woken up by someone, so it will just clean up and return. If
B occurs before A, schedule() has already put current to sleep, so the wakeup call won't be missed.
Of course, I will have to examine this issue carefully later on. Especially the implementation of
the schedule() function.

-- Mar 22nd 2014 --
This problem is now pretty clear. If wake_up*() is called before schedule(), 
it will set state to TASK_RUNNING and schedule() won't deactivate the task if it sees this. In this
case, schedule() will only make the task lay low for a while, the task is still on rq.

========

Mar 09th 2014

Can't find xtime initialization in time_init() or anywhere else.

========

Mar 10th 2014

I often find kernel routines executing payload functions that are registered. For example,
run_timer_softirq() executing a list of functions of expired timers. Should threads be spawned to
take care of these functions, or should they just be executed serially by the current thread for
simplicity? And which is the approach currently taken? I'll try to look into this.
-> in run_timer_softirq(), it just straight-up calls fn(data).

========

Mar 12th 2014

Tomorrow's work: take a good look at all the timer interrupt handler's routines (interval timers,
walltime update, process times, ...); examine NMI watchdog, clock time implementation.

========

Mar 13th 2014

smp_processor_id()'s main payload is in arch/x86/include/asm/percpu.h:
#ifdef CONFIG_X86_64
#define __percpu_seg		gs
#define __percpu_mov_op		movq
#else
#define __percpu_seg		fs
#define __percpu_mov_op		movl
#endif
#define __percpu_arg(x)		"%%"__stringify(__percpu_seg)":%P" #x
If !CONFIG_SMP then the macro will just return 0.

Load average calculation is quite a complex and mathematical process. I found a pretty good 
explanation at http://www.perfdynamics.com/CMG/CMGslides4up.pdf (saved to local). Basically its an
exponential weighting scheme. Samples (number of running & waiting processes) are taken every 5
seconds. The weight of each sample in the loadavg decreases along with its age. Note that, for
x-min loadavg (x = 1, 5, 15), samples taken x minutes before don't have the weight of 0. The number
of minutes is just an argument to the calculation (the decay rate), the weight of each sample only
hits zero if it has exponentially decreased to zero.

[2.6.34.7] has high-resolution timers, which I'll leave alone for now. It would take too much
effort to dig in raw (without any help from books).

========

Mar 14th 2014

In calibrate_delay(), loops_per_jiffy is initialized to (1<<12) before calculating. As how I see
it, the calculation takes it that lpj is at least 10^12. That is A LOT of MIPS, I think, and it
kinda bugs me! REALLY!

========

Mar 15th 2014

IDR... You f***ing b*tch.
First, it took me a whole lot of time.
Second, it pissed me off. find_next_bit() (in find_next_bit.c) is messed up. It doesn't return a m
that is equal to n.
Incase I come back to this, http://lwn.net/Articles/175432/.

-- Mar 26th 2014 --
Will write a module to test IDR later.

========

Mar 19th 2014

Seems like CONFIG_NO_HZ is related to system idle stuff.

========

Mar 20th 2014

The CFS might be simple in theory, but its details are quite messy and complicated to me. In some
places it it even inconsistent (just the presentation, it can't be technically flawed). I left out
some details but nevertheless got the overall mechanism of the CFS.
The one that gives me hell the most is the weighting of exec runtime!!

========

Mar 21st 2014

Posted my first question to the kernelnewbies mailing list. I asked about the effect of nice value
in the CFS scheduler. I wrote a simple program, running 2 instances of it with different nice 
values (on the same CPU) to test. I expected the execution speeds would be different, but they
seemed to be the same.

========

Mar 22nd 2014

struct task_struct has the following priority members: int prio, static_prio, normal_prio; 
unsigned int rt_priority. Found an OK explanation on the book Professional Linux Kernel 
Architecture. I think I'll download it.

A sample of acquiring locks in a predefined order to prevent deadlock is in double_rq_lock().

========

Mar 28th 2014

RCU is still a mystery to me.